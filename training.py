# -*- coding: utf-8 -*-
"""HANDIN_training.pynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/193Rr-lOdUFcmVatWkuNGDRBe2ZtCGr0z

# 0: Install Requirements
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install tensorflow_addons
!pip install vit_keras
!pip install pydot graphviz

import os
import time
import numpy as np
import pandas as pd
from tensorflow.keras.layers import LeakyReLU
from sklearn.utils.class_weight import compute_class_weight
import gc
from contextlib import contextmanager
from tensorflow.keras.layers import TimeDistributed, LSTM, Dense
from tensorflow.keras.models import Sequential
from vit_keras import vit
import cv2
import random
from vit_keras import vit
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, LeakyReLU, TimeDistributed, LSTM, GlobalAveragePooling1D, BatchNormalization, Conv3D, MaxPooling3D, Input
from tensorflow.keras.optimizers import Adam, AdamW
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler
from tensorflow.keras.utils import plot_model, to_categorical
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from sklearn.preprocessing import LabelBinarizer, LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from datetime import datetime
from vit_keras import vit
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Flatten, Dense
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from sklearn.utils.class_weight import compute_class_weight

print(tf.__version__)

"""# 1: Data Loading"""

frame_dir= '/content/drive/MyDrive/ThesisDSS/Thesis_V2/FINAL/data/_frms64_imgsze224'
extension = '_frms64_imgsze224'


# Create a dictionary of the calculated class weights
X_train = np.load(f'{frame_dir}/X_train{extension}.npy',allow_pickle=True)
y_train = np.load(f'{frame_dir}/y_train{extension}.npy',allow_pickle=True)

X_val = np.load(f'{frame_dir}/X_val{extension}.npy',allow_pickle=True)
y_val = np.load(f'{frame_dir}/y_val{extension}.npy',allow_pickle=True)

# X_test = np.load(f'{frame_dir}/X_test{extension}.npy',allow_pickle=True)
# y_test = np.load(f'{frame_dir}/y_test{extension}.npy',allow_pickle=True)

class_weight_dict = dict(enumerate(compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)))

class_labels = ['Hold Hands',
                'High Five',
                'Hug',
                'Fist Bump',
                'Arm Touch',
                'Shoulder Tap',
                'Elbow Bump',
                'Handshake', 'Control'
                ]

X_train = X_train[:, :60, :, :, :]
X_val = X_val[:, :60, :, :, :]

print("X_train shape:", X_train.shape)
print("X_train data type:", X_train.dtype)
print("y_train shape:", y_train.shape)
print("y_train data type:", y_train.dtype)
print('-'*50)
print("X_val shape:", X_val.shape)
print("X_val data type:", X_val.dtype)
print("y_val shape:", y_val.shape)
print("y_val data type:", y_val.dtype)
print('-'*50)

# print("X_test shape:", X_test.shape)
# print("X_test data type:", X_test.dtype)
# print("y_test shape:", y_test.shape)
# print("y_test data type:", y_test.dtype)

print(f"X_train Min pixel value: {X_train.min()}")
print(f"X_train Max pixel value: {X_train.max()}")
print(f"X_train Mean pixel value: {X_train.mean()}")
print('-'*50)

print(f"X_val Min pixel value: {X_val.min()}")
print(f"X_val Max pixel value: {X_val.max()}")
print(f"X_val Mean pixel value: {X_val.mean()}")

# print('-'*50)
# print(f"X_test Min pixel value: {X_test.min()}")
# print(f"X_test Max pixel value: {X_test.max()}")
# print(f"X_test Mean pixel value: {X_test.mean()}")

random.seed(23)
np.random.seed(23)
tf.random.set_seed(23)

label_encoder = LabelEncoder()
label_encoder.fit(y_train)

y_train = label_encoder.transform(y_train)
y_val = label_encoder.transform(y_val)

y_train = to_categorical(y_train, num_classes=len(label_encoder.classes_))
y_val = to_categorical(y_val, num_classes=len(label_encoder.classes_))

print("X_train shape:", X_train.shape)
print("X_train data type:", X_train.dtype)
print("y_train shape:", y_train.shape)
print("y_train data type:", y_train.dtype)
print('-'*50)
print("X_val shape:", X_val.shape)
print("X_val data type:", X_val.dtype)
print("y_val shape:", y_val.shape)
print("y_val data type:", y_val.dtype)
print('-'*50)

# print("X_test shape:", X_test.shape)
# print("X_test data type:", X_test.dtype)
# print("y_test shape:", y_test.shape)
# print("y_test data type:", y_test.dtype)

X_train = X_train.astype('float32') / 255.0
X_val = X_val.astype('float32') / 255.0

"""# 2. Define Model Functions"""

#! /usr/bin/python3

import math
import numpy as np

from tensorflow.keras.utils import Sequence

class DataGenerator(Sequence):

    def __init__(self, x, y, batch_size=32):
        self.x = np.asarray(x)
        self.y = np.asarray(y)
        self.batch_size = batch_size
        self.indices = np.arange(self.x.shape[0])
        np.random.default_rng().shuffle(self.indices)

    def __len__(self):
        return math.ceil(self.x.shape[0]/self.batch_size)

    def __getitem__(self, idx):
        ind = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]

        batch_x = self.x[ind]
        batch_y = self.y[ind]
        return batch_x, batch_y

    def on_epoch_end(self):
        np.random.shuffle(self.indices)


class PredictGenerator(Sequence):

    def __init__(self, x, batch_size=32):
        self.x = np.asarray(x)
        self.batch_size = batch_size
        self.indices = np.arange(self.x.shape[0])

    def __len__(self):
        return math.ceil(self.x.shape[0]/self.batch_size)

    def __getitem__(self, idx):
        ind = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]

        batch_x = self.x[ind]
        return batch_x

training_generator = DataGenerator(X_train, y_train, batch_size=16)
validation_generator = DataGenerator(X_val, y_val, batch_size=16)

from tensorflow import keras
from tensorflow.keras import layers
from vit_keras import vit

def ViTLSTM(LSTM_units, LSTM_layers):
    """
    Creates a combined Vision Transformer and LSTM Model for video classification.

    Arguments
    ---------
        sequence_length: int
            Length of the sequence of images (number of frames used)
        LSTM_units: int
            Number of units for the LSTM layers
        LSTM_layers: int
            Number of LSTM layers in the model
        categories: int
            Number of classification categories
        image_size: tuple
            Size of the images (height, width)€∑ß‹Ÿ

    Returns
    -------
        model: keras.Model
            Combined Vision Transformer and LSTM Model according to input specifications
    """

    vision_t = vit.vit_b16(image_size=(X_train.shape[2], X_train.shape[3]), activation='sigmoid', pretrained=True, include_top=False, pretrained_top=False)

    vision_t.trainable = False

    inputs = keras.Input(shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3], X_train.shape[4]))

    time_dist_vision_t = keras.layers.TimeDistributed(vision_t)(inputs)


    lstm = time_dist_vision_t
    for i in range(LSTM_layers):
        return_sequences = i < LSTM_layers - 1
        lstm = keras.layers.LSTM(LSTM_units, return_sequences=return_sequences)(lstm)
        lstm = keras.layers.Dropout(0.2)(lstm)

    outputs = keras.layers.Dense(9, activation='softmax')(lstm)

    model = keras.Model(inputs=inputs, outputs=outputs, name="vision_tLSTM_Model")

    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model

"""## 2.b.3. Function Resnet-LSTM"""

from tensorflow import keras
from tensorflow.keras import layers

def ResNetLSTM(LSTM_units, LSTM_layers):
    """
    Creates a combined ResNet and LSTM Model for video classification.

    Arguments
    ---------
        sequence_length: int
            Length of the sequence of images (number of frames used)
        LSTM_units: int
            Number of units for the LSTM layers
        LSTM_layers: int
            Number of LSTM layers in the model
        categories: int
            Number of classification categories (and number of units in Dense prediction layer)

    Returns
    -------
        model: keras.Model
            Combined ResNet and LSTM Model according to input specifications
    """

    resnet_model = keras.applications.ResNet50(include_top=False, weights='imagenet', pooling='avg')

    resnet_model.trainable = False

    inputs = keras.Input(shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3], X_train.shape[4]))

    time_dist_resnet = keras.layers.TimeDistributed(resnet_model)(inputs)

    lstm = time_dist_resnet
    for i in range(LSTM_layers):
        return_sequences = i < LSTM_layers - 1
        lstm = keras.layers.LSTM(LSTM_units, return_sequences=return_sequences)(lstm)
        lstm = keras.layers.Dropout(0.2)(lstm)

    outputs = keras.layers.Dense(9, activation='softmax')(lstm)

    model = keras.Model(inputs=inputs, outputs=outputs, name="ResNetLSTM_Model")

    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    return model

"""# 3. Modelling

## 3.1. ResNet-LSTM
Training ResNet-LSTM
"""

epochs = 50

# setting callbacks
early_stopping = EarlyStopping(patience=3,restore_best_weights=True)

# setting random seeds for reproducibility
random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

@contextmanager
def timer(title):
    start = time.time()
    yield
    end = time.time()
    print(f"{title}: {end - start} seconds")

def train_and_evaluate_model(layers, units):
    start_time_training = time.time()
    resnet_lstm_model = ResNetLSTM(LSTM_units=units, LSTM_layers=layers)
    print(resnet_lstm_model.summary())
    print(f'LSTM units {units}, LSTM layers: {layers}')
    history = resnet_lstm_model.fit(training_generator
                                    , validation_data=validation_generator
                                    , epochs = 1
                                   # , callbacks=[early_stopping]
                                    )
    end_time_training = time.time()
    training_time = (end_time_training- start_time_training)/60
    resnet_lstm_model.save(f'{model_save_dir}/model_ResNetLSTM_L{layers}_U{units}_one_epoch')
    history_data = history.history
    history_df = pd.DataFrame(history_data)
    history_df['epoch'] = range(1, len(history_df) + 1)
    history_df.to_csv(f'{model_save_dir}/model_ResNetLSTM_L{layers}_U{units}_history.csv', index=False)

    best_val_acc = max(history.history['val_accuracy'])
    best_val_loss = min(history.history['val_loss'])

    return {
        'Configuration_ResNetLSTM': f'L{layers}_U{units}',
        'Val_Accuracy': best_val_acc,
        'Val_Loss': best_val_loss,
        'Training_Time minuts': training_time
    }

def plot_results(results):
    df = pd.DataFrame(results)
    plt.figure(figsize=(10, 6))
    plt.bar(range(len(df)), df['Val_Accuracy'], align='center')
    plt.xticks(range(len(df)), df['Configuration_ResNetLSTM'], rotation=45)
    plt.xlabel('Model Configuration (Layers_Units)')
    plt.ylabel('Maximum Validation Accuracy')
    plt.title('ResNet-LSTM Validation Accuracy for Different Model Configurations')
    plt.tight_layout()
    plt.show()

val_results_resnetlstm = []

with timer("Total Tuning Time for ResNet-LSTM"):
    for layers in [1, 2, 4, 6]:
        for units in [32, 64, 128, 256, 512]:
            result = train_and_evaluate_model(layers, units)
            val_results_resnetlstm.append(result)
            gc.collect()

df = pd.DataFrame(val_results_resnetlstm)
df.to_csv(f'{model_save_dir}/ResNetLSTMmodel_val_results.csv', index=False)

np.save(f'{model_save_dir}/ResNetLSTMmodel_val_results.npy', df.to_numpy())

plot_results(val_results_resnetlstm)

"""## ViT-LSTM"""

epochs = 50

early_stopping = EarlyStopping(patience=3,restore_best_weights=True)

random.seed(42)
np.random.seed(42)
tf.random.set_seed(42)

@contextmanager
def timer(title):
    start = time.time()
    yield
    end = time.time()
    print(f"{title}: {end - start} seconds")

def train_and_evaluate_model(layers, units):
    start_time_training = time.time()
    ViT_LSTM_model = ViT_LSTM(LSTM_units=units, LSTM_layers=layers)
    print(ViT_LSTM_model.summary())
    print(f'LSTM units {units}, LSTM layers: {layers}')
    history = ViT_LSTM_model.fit(training_generator
                                    , validation_data=validation_generator
                                    , epochs = epochs
                                    , callbacks=[early_stopping]
                                    )
    end_time_training = time.time()
    training_time = (end_time_training- start_time_training)/60
    ViT_LSTM_model.save(f'{model_save_dir}/model_ViTLSTM_L{layers}_U{units}')
    history_data = history.history
    history_df = pd.DataFrame(history_data)
    history_df['epoch'] = range(1, len(history_df) + 1)
    history_df.to_csv(f'{model_save_dir}/model_ViTLSTM_L{layers}_U{units}_history.csv', index=False)

    best_val_acc = max(history.history['val_accuracy'])
    best_val_loss = min(history.history['val_loss'])

    return {
        'Configuration_ViTLSTM': f'L{layers}_U{units}',
        'Val_Accuracy': best_val_acc,
        'Val_Loss': best_val_loss,
        'Training_Time minuts': training_time
    }

def plot_results(results):
    df = pd.DataFrame(results)
    plt.figure(figsize=(10, 6))
    plt.bar(range(len(df)), df['Val_Accuracy'], align='center')
    plt.xticks(range(len(df)), df['Configuration_ViTLSTM'], rotation=45)
    plt.xlabel('Model Configuration (Layers_Units)')
    plt.ylabel('Maximum Validation Accuracy')
    plt.title('ViT-LSTM Validation Accuracy for Different Model Configurations')
    plt.tight_layout()
    plt.show()

val_results_vitlstm = []

with timer("Total Tuning Time for ViT-LSTM"):
    for layers in [1, 2, 4, 6]:
        for units in [32, 64, 128, 256, 512]:
            result = train_and_evaluate_model(layers, units)
            val_results_vitlstm.append(result)
            gc.collect()

df = pd.DataFrame(val_results_vitlstm)
df.to_csv(f'{model_save_dir}/ViTLSTMmodel_val_results.csv', index=False)

np.save(f'{model_save_dir}/ViTLSTMmodel_val_results.npy', df.to_numpy())

plot_results(val_results_vitlstm)